{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.layers as Layers\n",
    "import keras.optimizers as Optimizers \n",
    "from keras import Sequential\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Optimizers.Adam(learning_rate=0.01)\n",
    "\n",
    "    \n",
    "def createNewSeqModel():\n",
    "    model = Sequential()\n",
    "    weights = \"random_uniform\"\n",
    "    \n",
    "    model.add(Layers.Dense(50, input_shape=(1,), \n",
    "                          activation=\"relu\", kernel_initializer=weights))\n",
    "    model.add(Layers.Dense(50, activation=\"relu\",\n",
    "                           kernel_initializer=weights))\n",
    "    model.add(Layers.Dense(50, activation=\"linear\",\n",
    "                           kernel_initializer=weights))\n",
    "    model.compile(opt, loss=\"mse\")\n",
    "    return model    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createTrainngData(amount, maxNum, func):\n",
    "    inData = [random.randint(0, maxNum) for i in range(amount)]\n",
    "    results = [func(d) for d in inData]\n",
    "    return inData, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictWithModel(model, samples):\n",
    "    return model.predict(np.array(samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainModel(model, samples, labels, epochs, batchSize):\n",
    "    model.fit(np.array(samples), np.array(labels), epochs=epochs, batch_size=batchSize)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[77, 0, 39, 77, 36]\n",
      "[231, 0, 117, 231, 108]\n"
     ]
    }
   ],
   "source": [
    "func = lambda x: x*3\n",
    "samples, lables = createTrainngData(10000, 1000, func)\n",
    "evalSamples, evalLabels = createTrainngData(5, 100, func)\n",
    "\n",
    "print(evalSamples)\n",
    "print(evalLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions:  [[ 0.00957252 -0.06431103 -0.02817129 -0.00085377  0.1245234  -0.01707886\n",
      "   0.03233941  0.00112079  0.01104454  0.03808606 -0.04516651  0.0177221\n",
      "   0.03409555 -0.05829316 -0.06051874 -0.04517115  0.05907209 -0.02258671\n",
      "   0.03976759  0.0613218  -0.0264071  -0.03517789 -0.03127574  0.10315997\n",
      "  -0.05043451  0.0131074   0.05655979  0.10019246  0.02926775  0.05850716\n",
      "  -0.06750063  0.08940399  0.07921945  0.04531256  0.03145617  0.01199956\n",
      "   0.02482097  0.0295465   0.00968965 -0.0434709  -0.01661681 -0.0033764\n",
      "   0.03199464  0.09405084 -0.08382034 -0.09597494  0.08583744 -0.04285832\n",
      "  -0.09728515  0.00538326]\n",
      " [ 0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.        ]\n",
      " [ 0.00484843 -0.03257313 -0.01426857 -0.00043243  0.0630703  -0.00865033\n",
      "   0.0163797   0.00056767  0.00559399  0.01929034 -0.02287655  0.00897613\n",
      "   0.01726917 -0.02952511 -0.03065235 -0.0228789   0.02991963 -0.01144002\n",
      "   0.02014203  0.03105909 -0.01337503 -0.01781738 -0.01584096  0.05224986\n",
      "  -0.02554476  0.00663881  0.02864717  0.05074683  0.01482392  0.0296335\n",
      "  -0.03418863  0.04528254  0.04012414  0.02295052  0.01593234  0.0060777\n",
      "   0.01257166  0.01496511  0.00490775 -0.02201773 -0.00841631 -0.00171013\n",
      "   0.01620508  0.04763614 -0.04245446 -0.04861069  0.0434761  -0.02170747\n",
      "  -0.0492743   0.00272659]\n",
      " [ 0.00957252 -0.06431103 -0.02817129 -0.00085377  0.1245234  -0.01707886\n",
      "   0.03233941  0.00112079  0.01104454  0.03808606 -0.04516651  0.0177221\n",
      "   0.03409555 -0.05829316 -0.06051874 -0.04517115  0.05907209 -0.02258671\n",
      "   0.03976759  0.0613218  -0.0264071  -0.03517789 -0.03127574  0.10315997\n",
      "  -0.05043451  0.0131074   0.05655979  0.10019246  0.02926775  0.05850716\n",
      "  -0.06750063  0.08940399  0.07921945  0.04531256  0.03145617  0.01199956\n",
      "   0.02482097  0.0295465   0.00968965 -0.0434709  -0.01661681 -0.0033764\n",
      "   0.03199464  0.09405084 -0.08382034 -0.09597494  0.08583744 -0.04285832\n",
      "  -0.09728515  0.00538326]\n",
      " [ 0.00447547 -0.03006751 -0.01317099 -0.00039917  0.05821874 -0.00798492\n",
      "   0.01511973  0.000524    0.00516369  0.01780648 -0.02111681  0.00828566\n",
      "   0.01594078 -0.02725395 -0.02829448 -0.02111899  0.02761811 -0.01056002\n",
      "   0.01859264  0.02866994 -0.01234618 -0.01644681 -0.01462242  0.04823064\n",
      "  -0.02357978  0.00612814  0.02644354  0.04684323  0.01368362  0.027354\n",
      "  -0.03155873  0.04179928  0.03703768  0.0211851   0.01470678  0.00561019\n",
      "   0.01160461  0.01381395  0.00453023 -0.02032407 -0.0077689  -0.00157858\n",
      "   0.01495854  0.04397183 -0.03918873 -0.04487141  0.04013179 -0.02003767\n",
      "  -0.04548397  0.00251684]]\n",
      "Labels  [231, 0, 117, 231, 108]\n"
     ]
    }
   ],
   "source": [
    "myNet = createNewSeqModel()\n",
    "\n",
    "evalPred = predictWithModel(myNet, evalSamples)\n",
    "print(\"Predictions: \", evalPred)\n",
    "print(\"Labels \", evalLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "100/100 [==============================] - 0s 540us/step - loss: 412128.1562\n",
      "Epoch 2/100\n",
      "100/100 [==============================] - 0s 528us/step - loss: 10.3281\n",
      "Epoch 3/100\n",
      "100/100 [==============================] - 0s 533us/step - loss: 2.7353\n",
      "Epoch 4/100\n",
      "100/100 [==============================] - 0s 522us/step - loss: 2.5182\n",
      "Epoch 5/100\n",
      "100/100 [==============================] - 0s 532us/step - loss: 2.2934\n",
      "Epoch 6/100\n",
      "100/100 [==============================] - 0s 500us/step - loss: 2.0710\n",
      "Epoch 7/100\n",
      "100/100 [==============================] - 0s 521us/step - loss: 1.8319\n",
      "Epoch 8/100\n",
      "100/100 [==============================] - 0s 526us/step - loss: 1.5957\n",
      "Epoch 9/100\n",
      "100/100 [==============================] - 0s 531us/step - loss: 1.3748\n",
      "Epoch 10/100\n",
      "100/100 [==============================] - 0s 526us/step - loss: 1.1714\n",
      "Epoch 11/100\n",
      "100/100 [==============================] - 0s 515us/step - loss: 0.9919\n",
      "Epoch 12/100\n",
      "100/100 [==============================] - 0s 527us/step - loss: 0.8161\n",
      "Epoch 13/100\n",
      "100/100 [==============================] - 0s 499us/step - loss: 0.6786\n",
      "Epoch 14/100\n",
      "100/100 [==============================] - 0s 516us/step - loss: 0.5304\n",
      "Epoch 15/100\n",
      "100/100 [==============================] - 0s 529us/step - loss: 0.4224\n",
      "Epoch 16/100\n",
      "100/100 [==============================] - 0s 518us/step - loss: 0.3304\n",
      "Epoch 17/100\n",
      "100/100 [==============================] - 0s 508us/step - loss: 0.2540\n",
      "Epoch 18/100\n",
      "100/100 [==============================] - 0s 501us/step - loss: 0.1889\n",
      "Epoch 19/100\n",
      "100/100 [==============================] - 0s 517us/step - loss: 0.1402\n",
      "Epoch 20/100\n",
      "100/100 [==============================] - 0s 524us/step - loss: 0.1020\n",
      "Epoch 21/100\n",
      "100/100 [==============================] - 0s 528us/step - loss: 0.0754\n",
      "Epoch 22/100\n",
      "100/100 [==============================] - 0s 518us/step - loss: 0.0542\n",
      "Epoch 23/100\n",
      "100/100 [==============================] - 0s 511us/step - loss: 0.0344\n",
      "Epoch 24/100\n",
      "100/100 [==============================] - 0s 528us/step - loss: 0.2242\n",
      "Epoch 25/100\n",
      "100/100 [==============================] - 0s 511us/step - loss: 0.7458\n",
      "Epoch 26/100\n",
      "100/100 [==============================] - 0s 509us/step - loss: 0.0302\n",
      "Epoch 27/100\n",
      "100/100 [==============================] - 0s 531us/step - loss: 0.0065\n",
      "Epoch 28/100\n",
      "100/100 [==============================] - 0s 536us/step - loss: 0.0039\n",
      "Epoch 29/100\n",
      "100/100 [==============================] - 0s 519us/step - loss: 0.0024\n",
      "Epoch 30/100\n",
      "100/100 [==============================] - 0s 531us/step - loss: 0.0014\n",
      "Epoch 31/100\n",
      "100/100 [==============================] - 0s 515us/step - loss: 8.6775e-04\n",
      "Epoch 32/100\n",
      "100/100 [==============================] - 0s 527us/step - loss: 5.3952e-04\n",
      "Epoch 33/100\n",
      "100/100 [==============================] - 0s 527us/step - loss: 3.7376e-04\n",
      "Epoch 34/100\n",
      "100/100 [==============================] - 0s 526us/step - loss: 2.7753e-04\n",
      "Epoch 35/100\n",
      "100/100 [==============================] - 0s 510us/step - loss: 2.2204e-04\n",
      "Epoch 36/100\n",
      "100/100 [==============================] - 0s 490us/step - loss: 1.9632e-04\n",
      "Epoch 37/100\n",
      "100/100 [==============================] - 0s 528us/step - loss: 1.8465e-04\n",
      "Epoch 38/100\n",
      "100/100 [==============================] - 0s 534us/step - loss: 1.7928e-04\n",
      "Epoch 39/100\n",
      "100/100 [==============================] - 0s 527us/step - loss: 1.7661e-04\n",
      "Epoch 40/100\n",
      "100/100 [==============================] - 0s 517us/step - loss: 1.7565e-04\n",
      "Epoch 41/100\n",
      "100/100 [==============================] - 0s 529us/step - loss: 1.7504e-04\n",
      "Epoch 42/100\n",
      "100/100 [==============================] - 0s 534us/step - loss: 1.7466e-04\n",
      "Epoch 43/100\n",
      "100/100 [==============================] - 0s 531us/step - loss: 1.7452e-04\n",
      "Epoch 44/100\n",
      "100/100 [==============================] - 0s 521us/step - loss: 1.7434e-04\n",
      "Epoch 45/100\n",
      "100/100 [==============================] - 0s 554us/step - loss: 1.7423e-04\n",
      "Epoch 46/100\n",
      "100/100 [==============================] - 0s 574us/step - loss: 1.7406e-04\n",
      "Epoch 47/100\n",
      "100/100 [==============================] - 0s 542us/step - loss: 1.7866e-04\n",
      "Epoch 48/100\n",
      "100/100 [==============================] - 0s 568us/step - loss: 0.0020\n",
      "Epoch 49/100\n",
      "100/100 [==============================] - 0s 532us/step - loss: 683.1001\n",
      "Epoch 50/100\n",
      "100/100 [==============================] - 0s 514us/step - loss: 7.7613\n",
      "Epoch 51/100\n",
      "100/100 [==============================] - 0s 526us/step - loss: 0.0111\n",
      "Epoch 52/100\n",
      "100/100 [==============================] - 0s 514us/step - loss: 0.0022\n",
      "Epoch 53/100\n",
      "100/100 [==============================] - 0s 520us/step - loss: 6.2708e-04\n",
      "Epoch 54/100\n",
      "100/100 [==============================] - 0s 529us/step - loss: 4.1009e-04\n",
      "Epoch 55/100\n",
      "100/100 [==============================] - 0s 529us/step - loss: 0.0064\n",
      "Epoch 56/100\n",
      "100/100 [==============================] - 0s 526us/step - loss: 13.2759\n",
      "Epoch 57/100\n",
      "100/100 [==============================] - 0s 510us/step - loss: 402.8229\n",
      "Epoch 58/100\n",
      "100/100 [==============================] - 0s 517us/step - loss: 108.5472\n",
      "Epoch 59/100\n",
      "100/100 [==============================] - 0s 527us/step - loss: 0.0430\n",
      "Epoch 60/100\n",
      "100/100 [==============================] - 0s 531us/step - loss: 0.0029\n",
      "Epoch 61/100\n",
      "100/100 [==============================] - 0s 525us/step - loss: 0.0018\n",
      "Epoch 62/100\n",
      "100/100 [==============================] - 0s 534us/step - loss: 1.0901\n",
      "Epoch 63/100\n",
      "100/100 [==============================] - 0s 530us/step - loss: 45.0325\n",
      "Epoch 64/100\n",
      "100/100 [==============================] - 0s 528us/step - loss: 62.8959\n",
      "Epoch 65/100\n",
      "100/100 [==============================] - 0s 534us/step - loss: 370.0276\n",
      "Epoch 66/100\n",
      "100/100 [==============================] - 0s 534us/step - loss: 34.5183\n",
      "Epoch 67/100\n",
      "100/100 [==============================] - 0s 562us/step - loss: 0.0386\n",
      "Epoch 68/100\n",
      "100/100 [==============================] - 0s 571us/step - loss: 0.0016\n",
      "Epoch 69/100\n",
      "100/100 [==============================] - 0s 540us/step - loss: 0.0200\n",
      "Epoch 70/100\n",
      "100/100 [==============================] - 0s 523us/step - loss: 3.6030\n",
      "Epoch 71/100\n",
      "100/100 [==============================] - 0s 527us/step - loss: 436.4100\n",
      "Epoch 72/100\n",
      "100/100 [==============================] - 0s 535us/step - loss: 25.7803\n",
      "Epoch 73/100\n",
      "100/100 [==============================] - 0s 535us/step - loss: 0.0618\n",
      "Epoch 74/100\n",
      "100/100 [==============================] - 0s 529us/step - loss: 6.1414e-04\n",
      "Epoch 75/100\n",
      "100/100 [==============================] - 0s 526us/step - loss: 7.1196e-04\n",
      "Epoch 76/100\n",
      "100/100 [==============================] - 0s 536us/step - loss: 0.0024\n",
      "Epoch 77/100\n",
      "100/100 [==============================] - 0s 537us/step - loss: 709.5344\n",
      "Epoch 78/100\n",
      "100/100 [==============================] - 0s 580us/step - loss: 9.4406\n",
      "Epoch 79/100\n",
      "100/100 [==============================] - 0s 591us/step - loss: 0.0052\n",
      "Epoch 80/100\n",
      "100/100 [==============================] - 0s 577us/step - loss: 0.0019\n",
      "Epoch 81/100\n",
      "100/100 [==============================] - 0s 571us/step - loss: 7.5690e-04\n",
      "Epoch 82/100\n",
      "100/100 [==============================] - 0s 576us/step - loss: 8.1764e-04\n",
      "Epoch 83/100\n",
      "100/100 [==============================] - 0s 553us/step - loss: 0.0012\n",
      "Epoch 84/100\n",
      "100/100 [==============================] - 0s 591us/step - loss: 0.0591\n",
      "Epoch 85/100\n",
      "100/100 [==============================] - 0s 565us/step - loss: 112.3038\n",
      "Epoch 86/100\n",
      "100/100 [==============================] - 0s 546us/step - loss: 73.9466\n",
      "Epoch 87/100\n",
      "100/100 [==============================] - 0s 533us/step - loss: 220.0611\n",
      "Epoch 88/100\n",
      "100/100 [==============================] - 0s 540us/step - loss: 3.3925\n",
      "Epoch 89/100\n",
      "100/100 [==============================] - 0s 520us/step - loss: 0.1138\n",
      "Epoch 90/100\n",
      "100/100 [==============================] - 0s 524us/step - loss: 0.2340\n",
      "Epoch 91/100\n",
      "100/100 [==============================] - 0s 541us/step - loss: 350.8481\n",
      "Epoch 92/100\n",
      "100/100 [==============================] - 0s 531us/step - loss: 125.2366\n",
      "Epoch 93/100\n",
      "100/100 [==============================] - 0s 539us/step - loss: 0.0366\n",
      "Epoch 94/100\n",
      "100/100 [==============================] - 0s 511us/step - loss: 6.0554e-04\n",
      "Epoch 95/100\n",
      "100/100 [==============================] - 0s 533us/step - loss: 5.6262e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 96/100\n",
      "100/100 [==============================] - 0s 539us/step - loss: 0.0019\n",
      "Epoch 97/100\n",
      "100/100 [==============================] - 0s 510us/step - loss: 60.1034\n",
      "Epoch 98/100\n",
      "100/100 [==============================] - 0s 517us/step - loss: 187.4347\n",
      "Epoch 99/100\n",
      "100/100 [==============================] - 0s 527us/step - loss: 132.3783\n",
      "Epoch 100/100\n",
      "100/100 [==============================] - 0s 541us/step - loss: 125.0627\n"
     ]
    }
   ],
   "source": [
    "trainModel(myNet, samples, lables, 100, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions:  [[230.96567    230.96666    230.9662     230.96545    230.96527\n",
      "  230.96666    230.96515    230.96614    230.96577    230.96547\n",
      "  230.96608    230.96605    230.96562    230.96625    230.96643\n",
      "  230.96547    230.96535    230.96626    230.96585    230.96573\n",
      "  230.96559    230.96617    230.96565    230.96497    230.96594\n",
      "  230.96521    230.96486    230.96469    230.96526    230.96513\n",
      "  230.96634    230.96504    230.96509    230.96558    230.96489\n",
      "  230.96515    230.96564    230.96559    230.96626    230.96564\n",
      "  230.96652    230.96596    230.9653     230.96568    230.96777\n",
      "  230.96599    230.9653     230.96574    230.9669     230.96555   ]\n",
      " [  0.7139525    0.71441996   0.7151828    0.71428347   0.71308875\n",
      "    0.71411335   0.7135768    0.7133853    0.7147979    0.71392304\n",
      "    0.71486926   0.71378034   0.71410495   0.7141817    0.7140031\n",
      "    0.71405655   0.7132237    0.7132505    0.71342254   0.7126858\n",
      "    0.71327406   0.7138745    0.7148546    0.7130686    0.7138671\n",
      "    0.71365404   0.714037     0.7134087    0.7140834    0.71292585\n",
      "    0.714455     0.713853     0.71453094   0.7144613    0.71305716\n",
      "    0.71428204   0.7138959    0.7135308    0.713835     0.71378386\n",
      "    0.71328396   0.71256447   0.71477455   0.7132832    0.7145158\n",
      "    0.71519107   0.7131511    0.71328104   0.71436095   0.71310496]\n",
      " [116.967926   116.96898    116.968475   116.96771    116.96752\n",
      "  116.969025   116.96736    116.96843    116.968056   116.96772\n",
      "  116.96835    116.96834    116.96788    116.968506   116.96876\n",
      "  116.96771    116.967606   116.9686     116.968124   116.967995\n",
      "  116.96784    116.96846    116.96788    116.967155   116.96821\n",
      "  116.96744    116.967064   116.9669     116.96747    116.96734\n",
      "  116.96864    116.96727    116.96733    116.967804   116.96712\n",
      "  116.96734    116.967865   116.96787    116.96856    116.96789\n",
      "  116.96883    116.96823    116.9675     116.967964   116.970184\n",
      "  116.96827    116.96757    116.96801    116.96924    116.96779   ]\n",
      " [230.96567    230.96666    230.9662     230.96545    230.96527\n",
      "  230.96666    230.96515    230.96614    230.96577    230.96547\n",
      "  230.96608    230.96605    230.96562    230.96625    230.96643\n",
      "  230.96547    230.96535    230.96626    230.96585    230.96573\n",
      "  230.96559    230.96617    230.96565    230.96497    230.96594\n",
      "  230.96521    230.96486    230.96469    230.96526    230.96513\n",
      "  230.96634    230.96504    230.96509    230.96558    230.96489\n",
      "  230.96515    230.96564    230.96559    230.96626    230.96564\n",
      "  230.96652    230.96596    230.9653     230.96568    230.96777\n",
      "  230.96599    230.9653     230.96574    230.9669     230.96555   ]\n",
      " [107.96812    107.96916    107.96866    107.96788    107.96769\n",
      "  107.9692     107.96754    107.96863    107.96824    107.96789\n",
      "  107.96854    107.96854    107.96805    107.96869    107.96892\n",
      "  107.9679     107.96779    107.96877    107.9683     107.968155\n",
      "  107.96802    107.96863    107.96805    107.96733    107.96839\n",
      "  107.96762    107.96725    107.96709    107.96765    107.96752\n",
      "  107.96883    107.96747    107.96749    107.967995   107.967285\n",
      "  107.96752    107.96804    107.968056   107.96874    107.96806\n",
      "  107.96903    107.96842    107.967674   107.96814    107.97038\n",
      "  107.96845    107.96773    107.96818    107.96943    107.96796   ]]\n",
      "Labels  [231, 0, 117, 231, 108]\n"
     ]
    }
   ],
   "source": [
    "evalPred = predictWithModel(myNet, evalSamples)\n",
    "print(\"Predictions: \", evalPred)\n",
    "print(\"Labels \", evalLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
